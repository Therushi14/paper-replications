{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "74ItYh0mkGTf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"Implement rnn\"\n",
        "chars = list(set(data))\n",
        "data_size , vocab_size = len(data) , len(chars)\n",
        "\n",
        "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "print(f\"Data: {data}\")\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"Unique chars: {chars}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ87Zyn4kNPw",
        "outputId": "de5a7cd0-2ea8-43cc-9e6a-882a35f78f03"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: Implement rnn\n",
            "Vocab size: 9\n",
            "Unique chars: ['r', 'e', 'n', 'I', 'm', ' ', 't', 'l', 'p']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VaniallaRNN:\n",
        "  def __init__(self,vocab_size,hidden_size):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.learning_rate = 0.1\n",
        "\n",
        "    #Model Parameters (Weights and Biases)\n",
        "    #Xavier Initialization\n",
        "    self.Wxh = np.random.randn(hidden_size,vocab_size) * 0.01\n",
        "    self.Whh = np.random.randn(hidden_size,hidden_size) *0.01\n",
        "    self.Why = np.random.randn(vocab_size,hidden_size) *0.01\n",
        "    self.bh = np.zeros((hidden_size,1))\n",
        "    self.by = np.zeros((vocab_size,1))\n",
        "\n",
        "  def forward(self,inputs,h_prev):\n",
        "    xs,hs,ys,ps = {},{},{},{}\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for t in range(len(inputs)):\n",
        "      xs[t] = np.zeros((self.vocab_size,1))\n",
        "      xs[t][inputs[t]] = 1\n",
        "\n",
        "      hs[t] = np.tanh(np.dot(self.Wxh,xs[t]) + np.dot(self.Whh,hs[t-1]) + self.bh)\n",
        "      ys[t] = np.dot(self.Why,hs[t]) + self.by\n",
        "\n",
        "      # Numerically stable softmax\n",
        "      exp_y = np.exp(ys[t] - np.max(ys[t]))\n",
        "      ps[t] = exp_y / np.sum(exp_y)\n",
        "\n",
        "    return xs,hs,ps\n",
        "\n",
        "  def loss(self,ps,targets):\n",
        "      loss = 0\n",
        "      for t in range(len(targets)):\n",
        "        # Add a small epsilon to probabilities to prevent log(0) if needed, though stable softmax usually handles this.\n",
        "        loss += -np.log(ps[t][targets[t],0] + 1e-8)\n",
        "      return loss\n",
        "\n",
        "  def backward(self,xs,hs,ps,targets):\n",
        "    dWxh,dWhh,dWhy = np.zeros_like(self.Wxh) , np.zeros_like(self.Whh) , np.zeros_like(self.Why)\n",
        "    dbh,dby = np.zeros_like(self.bh) , np.zeros_like(self.by)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "    for t in reversed(range(len(targets))):\n",
        "\n",
        "      dy = np.copy(ps[t])\n",
        "      dy[targets[t]] -= 1\n",
        "\n",
        "      dWhy += np.dot(dy,hs[t].T)\n",
        "      dby += dy\n",
        "\n",
        "      dh = np.dot(self.Why.T,dy) + dhnext\n",
        "\n",
        "      dhraw = (1-hs[t] * hs[t]) *dh\n",
        "\n",
        "      dbh += dhraw\n",
        "      dWhh += np.dot(dhraw,hs[t-1].T)\n",
        "      dWxh += np.dot(dhraw,xs[t].T)\n",
        "\n",
        "      dhnext = np.dot(self.Whh.T,dhraw)\n",
        "\n",
        "    for dparam in [dWxh,dWhh,dWhy,dbh,dby]:\n",
        "      np.clip(dparam,-5,5,out=dparam)\n",
        "\n",
        "    return dWxh,dWhh,dWhy,dbh,dby\n",
        "\n",
        "  def update_params(self,dWxh,dWhh,dWhy,dbh,dby):\n",
        "    self.Wxh -= self.learning_rate * dWxh\n",
        "    self.Whh -= self.learning_rate * dWhh\n",
        "    self.Why -= self.learning_rate * dWhy\n",
        "    self.bh -= self.learning_rate * dbh\n",
        "    self.by -= self.learning_rate * dby"
      ],
      "metadata": {
        "id": "XNfTJmS0lTTp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "\n",
        "rnn = VaniallaRNN(vocab_size,hidden_size)\n",
        "\n",
        "input_indices = [char_to_ix[ch] for ch in data[:-1]]\n",
        "target_indices = [char_to_ix[ch] for ch in data[1:]]\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1))\n",
        "\n",
        "losses = [] # Initialize a list to store losses\n",
        "\n",
        "for i in range(2001):\n",
        "  xs,hs,ps = rnn.forward(input_indices,h_prev)\n",
        "\n",
        "  current_loss = rnn.loss(ps,target_indices)\n",
        "  losses.append(current_loss) # Store the loss\n",
        "\n",
        "  dWxh,dWhh,dWhy,dbh,dby = rnn.backward(xs,hs,ps,target_indices)\n",
        "\n",
        "  rnn.update_params(dWxh,dWhh,dWhy,dbh,dby)\n",
        "\n",
        "  h_prev = np.zeros((hidden_size,1))\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(f\"Iter {i}, loss: {current_loss:.4f}\")\n",
        "\n",
        "    seed_idx = input_indices[0]\n",
        "    x = np.zeros((vocab_size,1))\n",
        "    x[seed_idx] = 1\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    txt = ix_to_char[seed_idx]\n",
        "\n",
        "    for t in range(13):\n",
        "      h = np.tanh(np.dot(rnn.Wxh,x) + np.dot(rnn.Whh,h) + rnn.bh)\n",
        "      y = np.dot(rnn.Why,h) + rnn.by\n",
        "      # Numerically stable softmax for prediction\n",
        "      exp_y_pred = np.exp(y - np.max(y))\n",
        "      p = exp_y_pred / np.sum(exp_y_pred)\n",
        "      ix = np.random.choice(range(vocab_size),p = p.ravel())\n",
        "\n",
        "      x = np.zeros((vocab_size,1))\n",
        "      x[ix] = 1\n",
        "      txt += ix_to_char[ix]\n",
        "    print(f\"Predicition: {txt}\")\n",
        "\n",
        "print(f\"Final loss: {current_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3PYU9AmNFO",
        "outputId": "edd9c17b-c10f-473d-d51f-1eccebdf6b33"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0, loss: 26.3691\n",
            "Predicition: Ipm p tlntppem\n",
            "Iter 100, loss: 64.2706\n",
            "Predicition: Implementntntn\n",
            "Iter 200, loss: 37.2687\n",
            "Predicition: Implemenenrnrn\n",
            "Iter 300, loss: 54.9516\n",
            "Predicition: Impleme n n n \n",
            "Iter 400, loss: 48.5327\n",
            "Predicition: Implementntntn\n",
            "Iter 500, loss: 46.8698\n",
            "Predicition: Implemenrnrnrn\n",
            "Iter 600, loss: 54.4636\n",
            "Predicition: Impleme n n n \n",
            "Iter 700, loss: 49.3919\n",
            "Predicition: Implementntntn\n",
            "Iter 800, loss: 46.8378\n",
            "Predicition: Implemenrnrnrn\n",
            "Iter 900, loss: 54.8202\n",
            "Predicition: Impleme n n n \n",
            "Iter 1000, loss: 47.4842\n",
            "Predicition: Implementntntn\n",
            "Iter 1100, loss: 47.4102\n",
            "Predicition: Implemenrnrnrn\n",
            "Iter 1200, loss: 54.3697\n",
            "Predicition: Impleme n n n \n",
            "Iter 1300, loss: 49.1137\n",
            "Predicition: Implementntntn\n",
            "Iter 1400, loss: 47.6592\n",
            "Predicition: Implemenrnrnrn\n",
            "Iter 1500, loss: 54.3816\n",
            "Predicition: Impleme n n n \n",
            "Iter 1600, loss: 50.0968\n",
            "Predicition: Implementntntn\n",
            "Iter 1700, loss: 47.8714\n",
            "Predicition: Implemenrnrnrn\n",
            "Iter 1800, loss: 49.7485\n",
            "Predicition: Implementntntn\n",
            "Iter 1900, loss: 50.6333\n",
            "Predicition: Impleme n n n \n",
            "Iter 2000, loss: 53.7585\n",
            "Predicition: Implemenrnrnrn\n",
            "Final loss: 53.7585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VBrVLIcoBQ1"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}