{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbKWt1NvjwMmDn4+R6haW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Therushi14/paper-replications/blob/main/MLP_with_Backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "GxsthTAHOK0S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, x): return 1.0 / (1.0 + np.exp(-x))\n",
        "    def prime(self, x, out): return out * (1 - out)\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x): return np.maximum(0, x)\n",
        "    def prime(self, x, out): return (x > 0).astype(float)\n",
        "\n",
        "class MSE:\n",
        "    def loss(self, y_true, y_pred): return np.mean(0.5 * (y_pred - y_true)**2)\n",
        "    def prime(self, y_true, y_pred): return (y_pred - y_true)"
      ],
      "metadata": {
        "id": "PPX8cbdOOKw6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, lr=0.1):\n",
        "        self.layers = layers # e.g., [2, 3, 1]\n",
        "        self.lr = lr\n",
        "        self.params = {}\n",
        "        self.acts = [ReLU() for _ in range(len(layers)-2)] + [Sigmoid()] # ReLU hidden, Sigmoid output\n",
        "        self.loss_fn = MSE()\n",
        "\n",
        "        # Initialize Weights (Xavier) and Biases\n",
        "        for i in range(1, len(layers)):\n",
        "            n_in, n_out = layers[i-1], layers[i]\n",
        "            limit = np.sqrt(6 / (n_in + n_out))\n",
        "            self.params[f'W{i}'] = np.random.uniform(-limit, limit, (n_out, n_in))\n",
        "            self.params[f'b{i}'] = np.zeros((n_out, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        cache = {'A0': X}\n",
        "        for i in range(1, len(self.layers)):\n",
        "            Z = self.params[f'W{i}'] @ cache[f'A{i-1}'] + self.params[f'b{i}']\n",
        "            A = self.acts[i-1].forward(Z)\n",
        "            cache[f'Z{i}'], cache[f'A{i}'] = Z, A\n",
        "        return cache['A' + str(len(self.layers)-1)], cache\n",
        "\n",
        "    def backward(self, cache, Y):\n",
        "        L = len(self.layers) - 1\n",
        "        m = Y.shape[1]\n",
        "        gradients = {}\n",
        "\n",
        "        # Calculate Output Layer Error\n",
        "        A_final = cache[f'A{L}']\n",
        "        dA = self.loss_fn.prime(Y, A_final)\n",
        "\n",
        "        # Backpropagate\n",
        "        for i in range(L, 0, -1):\n",
        "            dZ = dA * self.acts[i-1].prime(cache[f'Z{i}'], cache[f'A{i}'])\n",
        "            gradients[f'dW{i}'] = (dZ @ cache[f'A{i-1}'].T) / m\n",
        "            gradients[f'db{i}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "            if i > 1:\n",
        "                dA = self.params[f'W{i}'].T @ dZ\n",
        "        return gradients\n",
        "\n",
        "    def update(self, grads):\n",
        "        for i in range(1, len(self.layers)):\n",
        "            self.params[f'W{i}'] -= self.lr * grads[f'dW{i}']\n",
        "            self.params[f'b{i}'] -= self.lr * grads[f'db{i}']\n",
        "\n",
        "    def train(self, X, Y, epochs=1000):\n",
        "        print(f\"Training on input shape: {X.shape}\")\n",
        "        for ep in range(epochs):\n",
        "            Y_hat, cache = self.forward(X)\n",
        "            loss = self.loss_fn.loss(Y, Y_hat)\n",
        "            grads = self.backward(cache, Y)\n",
        "            self.update(grads)\n",
        "            if ep % (epochs // 10) == 0:\n",
        "                print(f\"Epoch {ep} | Loss: {loss:.5f}\")"
      ],
      "metadata": {
        "id": "n3c4tgiAOKuN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # XOR Dataset: Inputs (2, 4) and Labels (1, 4)\n",
        "    X = np.array([[0, 0, 1, 1],\n",
        "                  [0, 1, 0, 1]])\n",
        "    Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "    # Create Network: 2 inputs -> 3 hidden neurons -> 1 output\n",
        "    nn = NeuralNetwork(layers=[2, 3, 1], lr=0.5)\n",
        "\n",
        "    # Train\n",
        "    nn.train(X, Y, epochs=5000)\n",
        "\n",
        "    # Test\n",
        "    prediction, _ = nn.forward(X)\n",
        "    print(\"\\nFinal Predictions (Rounded):\")\n",
        "    print(np.round(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnPD19RlOKre",
        "outputId": "6219131b-e71b-433e-d743-534b1502a61a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on input shape: (2, 4)\n",
            "Epoch 0 | Loss: 0.13990\n",
            "Epoch 500 | Loss: 0.01249\n",
            "Epoch 1000 | Loss: 0.00316\n",
            "Epoch 1500 | Loss: 0.00162\n",
            "Epoch 2000 | Loss: 0.00105\n",
            "Epoch 2500 | Loss: 0.00076\n",
            "Epoch 3000 | Loss: 0.00060\n",
            "Epoch 3500 | Loss: 0.00048\n",
            "Epoch 4000 | Loss: 0.00041\n",
            "Epoch 4500 | Loss: 0.00035\n",
            "\n",
            "Final Predictions (Rounded):\n",
            "[[0. 1. 1. 0.]]\n"
          ]
        }
      ]
    }
  ]
}